{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ronkow/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os           # os.path.join\n",
    "import numpy as np  # np.concatenate\n",
    "import pandas as pd\n",
    "\n",
    "import bow\n",
    "import utils\n",
    "\n",
    "#pd.get_dummies\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer  # fit_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GLOBAL CONSTANTS, DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_text9.csv, train_keyword9.csv are SMALL DATASETS used to test the code\n",
    "\n",
    "DATA_DIR = \"../data/\"\n",
    "\n",
    "DATA_FILE_TEXT = os.path.join(DATA_DIR, \"dataset_small/train_text9.csv\")           \n",
    "DATA_FILE_KEYWORD = os.path.join(DATA_DIR, \"dataset_small/train_keyword9.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(1) ONE HOT ENCODE TEXT** (TEST THE CODE USING SMALL DATASET)\n",
    "### **EXPLORE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"text\"\n",
      "\"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\"\n",
      "\"Forest fire near La Ronge Sask. Canada\"\n",
      "\"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\"\n",
      "\"13,000 people receive #wildfires evacuation orders in California \"\n",
      "\"Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school\"\n",
      "\"#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires\"\n",
      "\"#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas\"\n",
      "\"I'm on top of the hill and I can see a fire in the woods...\"\n",
      "\"There's an emergency evacuation happening now in the building across the street\"\n",
      "\n",
      "[['Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'], ['Forest fire near La Ronge Sask. Canada'], [\"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\"], ['13,000 people receive #wildfires evacuation orders in California '], ['Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school'], ['#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires'], ['#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas'], [\"I'm on top of the hill and I can see a fire in the woods...\"], [\"There's an emergency evacuation happening now in the building across the street\"]]\n"
     ]
    }
   ],
   "source": [
    "# CHECK\n",
    "s = utils.file_to_string(DATA_FILE_TEXT)\n",
    "print(s)\n",
    "\n",
    "text_list1 = utils.csv_to_list_of_lists(DATA_FILE_TEXT)\n",
    "print(text_list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RUN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#earthquak', 'fire', 'shelter', 'place', 'evacu', 'order', '#wildfir', 'california', '#alaska', '#rockyfir', '#cafir', '#flood', '#disast', 'street']\n",
      "\n",
      "['Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all', 'Forest fire near La Ronge Sask. Canada', \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\", '13,000 people receive #wildfires evacuation orders in California ', 'Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school', '#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires', '#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas', \"I'm on top of the hill and I can see a fire in the woods...\", \"There's an emergency evacuation happening now in the building across the street\"]\n",
      "\n",
      "[['#earthquak'], ['fire'], ['shelter', 'place', 'evacu', 'shelter', 'place', 'order'], ['#wildfir', 'evacu', 'order', 'california'], ['#alaska', '#wildfir'], ['#rockyfir', 'california', 'fire', '#cafir', '#wildfir'], ['#flood', '#disast', 'street'], ['fire'], ['evacu', 'street']]\n"
     ]
    }
   ],
   "source": [
    "#token_list = utils.csv_to_list_of_strings('data/tokens/train_text_token100.csv')\n",
    "token_list = utils.csv_to_list_of_strings('data/tokens_dataset_small/train_text_token.csv')\n",
    "print(token_list[0:20])\n",
    "print('')\n",
    "\n",
    "text_list = utils.csv_to_list_of_strings(DATA_FILE_TEXT)\n",
    "print(text_list)\n",
    "print('')\n",
    "\n",
    "text_token_list = bow.list_of_token_lists_text(text_list, token_list)\n",
    "print(text_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 1 1 1 0]\n",
      " [0 0 0 0 0 0 1 1 1 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 1 1 0 1 0 0 0 0]\n",
      " [0 0 1 0 1 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 1]]\n",
      "(9, 14)\n",
      "\n",
      "[['#alaska' '#cafir' '#disast' '#earthquak' '#flood' '#rockyfir'\n",
      "  '#wildfir' 'california' 'evacu' 'fire' 'order' 'place' 'shelter'\n",
      "  'street']]\n",
      "(1, 14)\n",
      "\n",
      "[['#alaska' '#cafir' '#disast' '#earthquak' '#flood' '#rockyfir'\n",
      "  '#wildfir' 'california' 'evacu' 'fire' 'order' 'place' 'shelter'\n",
      "  'street']\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 1 1 1 0]\n",
      " [0 0 0 0 0 0 1 1 1 0 1 0 0 0]\n",
      " [1 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 1 1 0 1 0 0 0 0]\n",
      " [0 0 1 0 1 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 1]]\n",
      "(10, 14)\n"
     ]
    }
   ],
   "source": [
    "onehot_multi_text = MultiLabelBinarizer()\n",
    "\n",
    "onehot_text_nolabel = onehot_multi_text.fit_transform(text_token_list)\n",
    "num_row, num_col = onehot_text_nolabel.shape\n",
    "\n",
    "print(onehot_text_nolabel)\n",
    "print(onehot_text_nolabel.shape)\n",
    "#print(type(onehot_text_nolabel))\n",
    "print('')\n",
    "\n",
    "text_labels = onehot_multi_text.classes_\n",
    "text_labels = text_labels.reshape(1, num_col)\n",
    "\n",
    "print(text_labels)\n",
    "print(text_labels.shape)\n",
    "#print(type(text_labels))\n",
    "print('')\n",
    "\n",
    "onehot_text = np.concatenate((text_labels,onehot_text_nolabel), axis=0)\n",
    "print(onehot_text)\n",
    "print(onehot_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ONE HOT ENCODE KEYWORDS** (USING SMALL DATASET)  \n",
    "### **EXPLORE DATA** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"keyword\"\n",
      "\"earthquake\"\n",
      "\"forestfires\"\n",
      "\"evacuation\"\n",
      "\"wildfires\"\n",
      "\"wildfires\"\n",
      "\"wildfire\"\n",
      "\"flood\"\n",
      "\"fire\"\n",
      "\"evacuations\"\n",
      "\n",
      "\n",
      "['kw_keyword', 'kw_earthquak', 'kw_forestfir', 'kw_evacu', 'kw_wildfir', 'kw_wildfir', 'kw_wildfir', 'kw_flood', 'kw_fire', 'kw_evacu']\n"
     ]
    }
   ],
   "source": [
    "# CHECK\n",
    "\n",
    "s = utils.file_to_string(DATA_FILE_KEYWORD)\n",
    "print(s)\n",
    "print('')\n",
    "\n",
    "keyword_list1 = bow.clean_doc_keyword(s)\n",
    "print(keyword_list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RUN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['earthquake', 'forestfires', 'evacuation', 'wildfires', 'wildfires', 'wildfire', 'flood', 'fire', 'evacuations']\n",
      "\n",
      "[['kw_earthquak'], ['kw_forestfir'], ['kw_evacu'], ['kw_wildfir'], ['kw_wildfir'], ['kw_wildfir'], ['kw_flood'], ['kw_fire'], ['kw_evacu']]\n"
     ]
    }
   ],
   "source": [
    "keyword_list = utils.csv_to_list_of_strings(DATA_FILE_KEYWORD)\n",
    "print(keyword_list)\n",
    "print('')\n",
    "\n",
    "keyword_token_list = bow.list_of_token_lists_keyword(keyword_list)\n",
    "print(keyword_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 1 0 0 0 0]]\n",
      "(9, 6)\n",
      "\n",
      "[['kw_earthquak' 'kw_evacu' 'kw_fire' 'kw_flood' 'kw_forestfir'\n",
      "  'kw_wildfir']]\n",
      "(1, 6)\n",
      "\n",
      "[['kw_earthquak' 'kw_evacu' 'kw_fire' 'kw_flood' 'kw_forestfir'\n",
      "  'kw_wildfir']\n",
      " [1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 1 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 1 0 0 0 0]]\n",
      "(10, 6)\n"
     ]
    }
   ],
   "source": [
    "onehot_multi_keyword = MultiLabelBinarizer()\n",
    "onehot_keyword_nolabel = onehot_multi_keyword.fit_transform(keyword_token_list)\n",
    "num_row, num_col = onehot_keyword_nolabel.shape\n",
    "\n",
    "print(onehot_keyword_nolabel)\n",
    "print(onehot_keyword_nolabel.shape)\n",
    "#print(type(onehot_keyword_nolabel))\n",
    "print('')\n",
    "\n",
    "keyword_labels = onehot_multi_keyword.classes_\n",
    "keyword_labels = keyword_labels.reshape(1, num_col)\n",
    "\n",
    "print(keyword_labels)\n",
    "print(keyword_labels.shape)\n",
    "#print(type(keyword_labels))\n",
    "print('')\n",
    "\n",
    "onehot_keyword = np.concatenate((keyword_labels,onehot_keyword_nolabel), axis=0)\n",
    "print(onehot_keyword)\n",
    "print(onehot_keyword.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CONCATENATE KEYWORD AND TEXT ONE HOT DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['kw_earthquak' 'kw_evacu' 'kw_fire' 'kw_flood' 'kw_forestfir'\n",
      "  'kw_wildfir' '#alaska' '#cafir' '#disast' '#earthquak' '#flood'\n",
      "  '#rockyfir' '#wildfir' 'california' 'evacu' 'fire' 'order' 'place'\n",
      "  'shelter' 'street']\n",
      " [1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1]]\n",
      "(10, 20)\n"
     ]
    }
   ],
   "source": [
    "onehot_train = np.concatenate((onehot_keyword,onehot_text), axis=1)\n",
    "print(onehot_train)\n",
    "print(onehot_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CONVERT NUMPY ARRAY TO DATAFRAME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kw_earthquak</th>\n",
       "      <th>kw_evacu</th>\n",
       "      <th>kw_fire</th>\n",
       "      <th>kw_flood</th>\n",
       "      <th>kw_forestfir</th>\n",
       "      <th>kw_wildfir</th>\n",
       "      <th>#alaska</th>\n",
       "      <th>#cafir</th>\n",
       "      <th>#disast</th>\n",
       "      <th>#earthquak</th>\n",
       "      <th>#flood</th>\n",
       "      <th>#rockyfir</th>\n",
       "      <th>#wildfir</th>\n",
       "      <th>california</th>\n",
       "      <th>evacu</th>\n",
       "      <th>fire</th>\n",
       "      <th>order</th>\n",
       "      <th>place</th>\n",
       "      <th>shelter</th>\n",
       "      <th>street</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  kw_earthquak kw_evacu kw_fire kw_flood kw_forestfir kw_wildfir #alaska  \\\n",
       "0            1        0       0        0            0          0       0   \n",
       "1            0        0       0        0            1          0       0   \n",
       "2            0        1       0        0            0          0       0   \n",
       "3            0        0       0        0            0          1       0   \n",
       "4            0        0       0        0            0          1       1   \n",
       "5            0        0       0        0            0          1       0   \n",
       "6            0        0       0        1            0          0       0   \n",
       "7            0        0       1        0            0          0       0   \n",
       "8            0        1       0        0            0          0       0   \n",
       "\n",
       "  #cafir #disast #earthquak #flood #rockyfir #wildfir california evacu fire  \\\n",
       "0      0       0          1      0         0        0          0     0    0   \n",
       "1      0       0          0      0         0        0          0     0    1   \n",
       "2      0       0          0      0         0        0          0     1    0   \n",
       "3      0       0          0      0         0        1          1     1    0   \n",
       "4      0       0          0      0         0        1          0     0    0   \n",
       "5      1       0          0      0         1        1          1     0    1   \n",
       "6      0       1          0      1         0        0          0     0    0   \n",
       "7      0       0          0      0         0        0          0     0    1   \n",
       "8      0       0          0      0         0        0          0     1    0   \n",
       "\n",
       "  order place shelter street  \n",
       "0     0     0       0      0  \n",
       "1     0     0       0      0  \n",
       "2     1     1       1      0  \n",
       "3     1     0       0      0  \n",
       "4     0     0       0      0  \n",
       "5     0     0       0      0  \n",
       "6     0     0       0      1  \n",
       "7     0     0       0      0  \n",
       "8     0     0       0      1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = utils.np_array_to_dataframe(onehot_train)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE_BOW = os.path.join(DATA_DIR, \"bow/train_bow9.csv\")\n",
    "\n",
    "utils.dataframe_to_csv(train_df, DATA_FILE_BOW)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
