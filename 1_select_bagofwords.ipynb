{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os            # os.path.join\n",
    "import re            # re.sub\n",
    "import pandas as pd  # pd.DataFrame\n",
    "\n",
    "#from itertools import islice   # islice(iterable, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ronkow/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DATA, GLOBAL CONSTANTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNC = '''!?.,;:$&*^~_\"`(){}[]/\\|<>=%+-'''  # exclude # (for #user) and @ (for @hashtag) and ' (so that can't is not converted to cant)\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english')) # returns a set of stop words\n",
    "ADD_WORDS = {\"i'm\"}\n",
    "STOP_WORDS = STOP_WORDS.union(ADD_WORDS)\n",
    "\n",
    "DATA_DIR = \"data/\"\n",
    "DATA_FILE = os.path.join(DATA_DIR, \"train_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'themselves', 'she', 'then', \"doesn't\", 'because', 'for', 'while', 'yourself', \"shouldn't\", 'not', 'or', \"it's\", 'having', 'other', 'after', 'again', 'from', 'which', \"hadn't\", 'ma', 'couldn', 'very', 'her', 'herself', 'does', 't', 'the', 'their', 're', 'ourselves', 'further', 'been', 'yourselves', 'of', 'don', \"isn't\", 'now', \"couldn't\", 'ours', 'this', \"needn't\", 'whom', 'any', 'during', 'but', 's', 'them', 'and', 'is', \"aren't\", 'what', 'most', 'only', 'doesn', 'it', 'myself', 'ain', \"won't\", 'weren', 'we', 'theirs', 'same', 'that', \"weren't\", 'won', 'in', 'you', 'his', 'i', 've', \"mightn't\", 'itself', 'just', 'few', 'be', 'if', 'under', 'hadn', 'him', 'wasn', 'below', 'there', \"hasn't\", 'nor', 'were', 'here', 'are', 'some', 'no', 'before', 'he', \"you're\", 'its', 'have', 'at', 'mustn', 'didn', \"that'll\", 'off', 'should', 'wouldn', 'where', 'll', 'haven', 'into', 'as', 'our', 'your', \"don't\", 'until', 'how', \"haven't\", 'once', 'shan', 'mightn', \"didn't\", \"mustn't\", 'shouldn', 'had', \"you'll\", 'my', 'about', 'o', 'between', 'all', 'both', 'over', \"you'd\", 'isn', \"shan't\", 'hers', 'so', 'has', 'more', 'did', 'against', 'who', 'by', 'when', \"should've\", \"you've\", 'each', 'such', 'me', 'a', 'out', 'those', 'an', 'down', 'am', 'hasn', 'on', 'why', 'needn', 'with', 'was', \"wasn't\", \"wouldn't\", 'through', 'too', \"i'm\", 'will', \"she's\", 'they', 'y', 'own', 'can', 'himself', 'm', 'these', 'than', 'doing', 'above', 'd', 'up', 'being', 'do', 'yours', 'to', 'aren'}\n"
     ]
    }
   ],
   "source": [
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filepath):\n",
    "    \"\"\"\n",
    "    ARGUMENT: file path\n",
    "    RETURN: string of text from file\n",
    "    \"\"\"\n",
    "    with open(filepath) as f:\n",
    "        s = f.read()           # \n",
    "    return s                   # string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc_text(doc):\n",
    "    \"\"\"\n",
    "    ARGUMENT: text (string)\n",
    "    RETURN: list of tokens\n",
    "    \"\"\"\n",
    "    doc = doc.replace('...',' ... ')  # to avoid converting abc...xyz to abcxyz\n",
    "    doc = doc.replace(\"'\",' ')        # to convert \"can't\" to \"can\" and \"t\"\n",
    "    \n",
    "    for p in PUNC:\n",
    "        doc = doc.replace(p,'')\n",
    "  \n",
    "    tokens = doc.split()                                             # returns a list of tokens\n",
    "    tokens = [w.lower() for w in tokens]                             # convert all letters to lower case  \n",
    "    tokens = [w for w in tokens if not w in STOP_WORDS]              # exclude stop words\n",
    "    \n",
    "    tokens = [w for w in tokens if not w.isdigit()]                  # exclude all numbers, but include words with numbers, such as abc12\n",
    "\n",
    "    tokens = [porter.stem(w) for w in tokens]                        # stemming\n",
    "    tokens = [w for w in tokens if len(w)>=2]                        # include only words with length >= 2\n",
    "    \n",
    "    return tokens                                                    # list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(tokens):\n",
    "    \"\"\"\n",
    "    ARGUMENT: list of tokens\n",
    "    RETURN: dictionary {token:count}\n",
    "    \"\"\"\n",
    "    token_count_dict = dict()\n",
    "    for w in tokens:\n",
    "        token_count_dict[w] = token_count_dict.get(w,0) + 1\n",
    "        \n",
    "    return token_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduced_dict(count_dict, N):\n",
    "    \"\"\"\n",
    "    ARGUMENTS: dictionary of counts {x:count}, N\n",
    "    RETURN: reduced dictionary of counts, count >= N\n",
    "    \"\"\"\n",
    "    reduced_token_count_dict = dict()\n",
    "    token_list = []\n",
    "    \n",
    "    for w in count_dict:\n",
    "        if count_dict[w] >= N:\n",
    "            reduced_token_count_dict[w] = count_dict[w]\n",
    "            token_list.append(w)\n",
    "            \n",
    "    return reduced_token_count_dict, token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints dict items in descending count order\n",
    "\n",
    "def print_token_count(count_dict, N):   \n",
    "    \"\"\"\n",
    "    ARGUMENTS: dictionary of counts {x: count}, N\n",
    "    prints top N key-value in dictionary\n",
    "    \"\"\"\n",
    "    for w in sorted(count_dict, key = count_dict.get, reverse = True):\n",
    "        if count_dict[w] >= N:\n",
    "            #print(f'{w}:{token_count_dict[w]}',sep=' ', end=' ', flush=True)\n",
    "            print(f'{w}:{token_count_dict[w]}  ', end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to save tokens list as csv\n",
    "\n",
    "def list_to_csv(list, filepath):\n",
    "    \"\"\"\n",
    "    ARGUMENT: list, file path\n",
    "    RETURN: csv file\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(list, columns=[\"text_tokens\"])\n",
    "    df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RUN!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2944\n",
      "2069\n",
      "1602\n",
      "1441\n",
      "1053\n",
      "986\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "# The number at the end of the variable refers to the token count\n",
    "# DATA_TOKEN4 means all tokens with counts >= 4\n",
    "\n",
    "DATA_TOKEN4 = os.path.join(DATA_DIR, \"tokens/train_text_token4.csv\")\n",
    "DATA_TOKEN6 = os.path.join(DATA_DIR, \"tokens/train_text_token6.csv\")\n",
    "DATA_TOKEN8 = os.path.join(DATA_DIR, \"tokens/train_text_token8.csv\")\n",
    "DATA_TOKEN9 = os.path.join(DATA_DIR, \"tokens/train_text_token9.csv\")\n",
    "DATA_TOKEN13 = os.path.join(DATA_DIR, \"tokens/train_text_token13.csv\")\n",
    "DATA_TOKEN14 = os.path.join(DATA_DIR, \"tokens/train_text_token14.csv\")\n",
    "DATA_TOKEN100 = os.path.join(DATA_DIR, \"tokens/train_text_token100.csv\")\n",
    "\n",
    "doc = load_doc(DATA_FILE)\n",
    "tokens = clean_doc_text(doc)\n",
    "token_count_dict = count_tokens(tokens[1:])\n",
    "\n",
    "# TOKENS WITH COUNT >= 4: 2944 TOKENS\n",
    "reduced_token_count_dict4, token_list4 = reduced_dict(token_count_dict, 4)\n",
    "list_to_csv(token_list4, DATA_TOKEN4)\n",
    "print(len(token_list4))\n",
    "\n",
    "# TOKENS WITH COUNT >= 6: 2069 TOKENS\n",
    "reduced_token_count_dict6, token_list6 = reduced_dict(token_count_dict, 6)\n",
    "list_to_csv(token_list6, DATA_TOKEN6)\n",
    "print(len(token_list6))\n",
    "\n",
    "# TOKENS WITH COUNT >= 8: 1602 TOKENS\n",
    "reduced_token_count_dict8, token_list8 = reduced_dict(token_count_dict, 8)\n",
    "list_to_csv(token_list8, DATA_TOKEN8)\n",
    "print(len(token_list8))\n",
    "\n",
    "# TOKENS WITH COUNT >= 9: 1441 TOKENS\n",
    "reduced_token_count_dict9, token_list9 = reduced_dict(token_count_dict, 9)\n",
    "list_to_csv(token_list9, DATA_TOKEN9)\n",
    "print(len(token_list9))\n",
    "\n",
    "# TOKENS WITH COUNT >= 13: 1053 TOKENS\n",
    "reduced_token_count_dict13, token_list13 = reduced_dict(token_count_dict, 13)\n",
    "list_to_csv(token_list13, DATA_TOKEN13)\n",
    "print(len(token_list13))\n",
    "\n",
    "# TOKENS WITH COUNT >= 14: 986 TOKENS\n",
    "reduced_token_count_dict14, token_list14 = reduced_dict(token_count_dict, 14)\n",
    "list_to_csv(token_list14, DATA_TOKEN14)\n",
    "print(len(token_list14))\n",
    "\n",
    "# TOKENS WITH COUNT >= 100: 64 TOKENS\n",
    "reduced_token_count_dict100, token_list100 = reduced_dict(token_count_dict, 100)\n",
    "list_to_csv(token_list100, DATA_TOKEN100)\n",
    "print(len(token_list100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
