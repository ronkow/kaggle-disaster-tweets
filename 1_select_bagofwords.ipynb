{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ronkow/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os            # os.path.join\n",
    "import re            # re.sub\n",
    "import pandas as pd  # pd.DataFrame\n",
    "\n",
    "import bow\n",
    "import utils\n",
    "\n",
    "#from itertools import islice   # islice(iterable, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DATA, GLOBAL CONSTANTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/\"\n",
    "DATA_FILE = os.path.join(DATA_DIR, \"train_text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(tokens):\n",
    "    \"\"\"\n",
    "    ARGUMENT: list of tokens\n",
    "    RETURN: dictionary {token:count}\n",
    "    \"\"\"\n",
    "    token_count_dict = dict()\n",
    "    for w in tokens:\n",
    "        token_count_dict[w] = token_count_dict.get(w,0) + 1\n",
    "        \n",
    "    return token_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduced_dict(count_dict, N):\n",
    "    \"\"\"\n",
    "    ARGUMENTS: dictionary of counts {x:count}, N\n",
    "    RETURN: reduced dictionary of counts, count >= N\n",
    "    \"\"\"\n",
    "    reduced_token_count_dict = dict()\n",
    "    token_list = []\n",
    "    \n",
    "    for w in count_dict:\n",
    "        if count_dict[w] >= N:\n",
    "            reduced_token_count_dict[w] = count_dict[w]\n",
    "            token_list.append(w)\n",
    "            \n",
    "    return reduced_token_count_dict, token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints dict items in descending count order\n",
    "\n",
    "def print_token_count(count_dict, N):   \n",
    "    \"\"\"\n",
    "    ARGUMENTS: dictionary of counts {x: count}, N\n",
    "    prints top N key-value in dictionary\n",
    "    \"\"\"\n",
    "    for w in sorted(count_dict, key = count_dict.get, reverse = True):\n",
    "        if count_dict[w] >= N:\n",
    "            #print(f'{w}:{token_count_dict[w]}',sep=' ', end=' ', flush=True)\n",
    "            print(f'{w}:{token_count_dict[w]}  ', end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RUN!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"text\"\n",
      "\"Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\"\n",
      "\"Forest fire near La Ronge Sask. Canada\"\n",
      "\"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\"\n",
      "\"13,000 people receive #wildfires evacuation orders in California \"\n",
      "\"Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school\"\n",
      "\"#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires\"\n",
      "\"#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas\"\n",
      "\"I'm on top of the hill and I can see a fire in the woods...\"\n",
      "\"There's an emergency evacuation happening now in the building across the street\"\n",
      "\"I'm afraid that the tornado is coming to our area...\"\n",
      "\"Three people died from the heat wave so far\"\n",
      "\"Haha South Tampa is getting flooded hah- WAIT A SECOND I LIVE IN SOUTH TAMPA WHAT AM I GONNA DO WHAT AM I GONNA DO FVCK #flooding\n"
     ]
    }
   ],
   "source": [
    "doc = utils.file_to_string(DATA_FILE)\n",
    "print(doc[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'deed', 'reason', '#earthquak', 'may', 'allah', 'forgiv', 'us', 'forest', 'fire']\n"
     ]
    }
   ],
   "source": [
    "tokens = bow.clean_doc_text(doc)\n",
    "print(tokens[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2944\n",
      "2070\n",
      "1603\n",
      "1442\n",
      "1053\n",
      "987\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "# The number at the end of the variable refers to the token count\n",
    "# DATA_TOKEN4 means all tokens with counts >= 4\n",
    "\n",
    "DATA_TOKEN4 = os.path.join(DATA_DIR, \"tokens/train_text_token4.csv\")\n",
    "DATA_TOKEN6 = os.path.join(DATA_DIR, \"tokens/train_text_token6.csv\")\n",
    "DATA_TOKEN8 = os.path.join(DATA_DIR, \"tokens/train_text_token8.csv\")\n",
    "DATA_TOKEN9 = os.path.join(DATA_DIR, \"tokens/train_text_token9.csv\")\n",
    "DATA_TOKEN13 = os.path.join(DATA_DIR, \"tokens/train_text_token13.csv\")\n",
    "DATA_TOKEN14 = os.path.join(DATA_DIR, \"tokens/train_text_token14.csv\")\n",
    "DATA_TOKEN100 = os.path.join(DATA_DIR, \"tokens/train_text_token100.csv\")\n",
    "\n",
    "\n",
    "\n",
    "token_count_dict = count_tokens(tokens[1:])\n",
    "\n",
    "# TOKENS WITH COUNT >= 4: 2944 TOKENS\n",
    "reduced_token_count_dict4, token_list4 = reduced_dict(token_count_dict, 4)\n",
    "utils.list_to_csv(token_list4, DATA_TOKEN4)\n",
    "print(len(token_list4))\n",
    "\n",
    "# TOKENS WITH COUNT >= 6: 2070 TOKENS\n",
    "reduced_token_count_dict6, token_list6 = reduced_dict(token_count_dict, 6)\n",
    "utils.list_to_csv(token_list6, DATA_TOKEN6)\n",
    "print(len(token_list6))\n",
    "\n",
    "# TOKENS WITH COUNT >= 8: 1603 TOKENS\n",
    "reduced_token_count_dict8, token_list8 = reduced_dict(token_count_dict, 8)\n",
    "utils.list_to_csv(token_list8, DATA_TOKEN8)\n",
    "print(len(token_list8))\n",
    "\n",
    "# TOKENS WITH COUNT >= 9: 1442 TOKENS\n",
    "reduced_token_count_dict9, token_list9 = reduced_dict(token_count_dict, 9)\n",
    "utils.list_to_csv(token_list9, DATA_TOKEN9)\n",
    "print(len(token_list9))\n",
    "\n",
    "# TOKENS WITH COUNT >= 13: 1053 TOKENS\n",
    "reduced_token_count_dict13, token_list13 = reduced_dict(token_count_dict, 13)\n",
    "utils.list_to_csv(token_list13, DATA_TOKEN13)\n",
    "print(len(token_list13))\n",
    "\n",
    "# TOKENS WITH COUNT >= 14: 987 TOKENS\n",
    "reduced_token_count_dict14, token_list14 = reduced_dict(token_count_dict, 14)\n",
    "utils.list_to_csv(token_list14, DATA_TOKEN14)\n",
    "print(len(token_list14))\n",
    "\n",
    "# TOKENS WITH COUNT >= 100: 64 TOKENS\n",
    "reduced_token_count_dict100, token_list100 = reduced_dict(token_count_dict, 100)\n",
    "utils.list_to_csv(token_list100, DATA_TOKEN100)\n",
    "print(len(token_list100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
